{"cells":[{"metadata":{},"cell_type":"markdown","source":"References/Credits: \n* https://www.kaggle.com/ateplyuk/pytorch-efficientnet\n* https://www.kaggle.com/chanhu/eye-efficientnet-pytorch-lb-0-777/data\n* https://www.kaggle.com/abhinand05/blindness-detection-complete-pytorch-training\n* https://www.kaggle.com/ratthachat/aptos-eye-preprocessing-in-diabetic-retinopathy"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport random\nimport cv2\nimport seaborn as sns\nfrom PIL import Image\n\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\nimport torch.utils.data as utils\n\nimport torchvision\nfrom torchvision import transforms\n\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_dir = '../input/aptos2019-blindness-detection/'\ntrain_dir = data_dir + '/train_images/'\ntest_dir = data_dir + '/test_images/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(data_dir+\"train.csv\")\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMG_SIZE = 224\nseed_everything(13)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N_FOLDS = 3\nBATCH_SIZE = 32\nN_EPOCHS = 7","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"folds = StratifiedKFold(n_splits=N_FOLDS, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"counts = train_df['diagnosis'].value_counts()\nclass_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']\nfor i,x in enumerate(class_list):\n    counts[x] = counts.pop(i)\n\nplt.figure(figsize=(10,5))\nsns.barplot(counts.index, counts.values, alpha=0.8, palette='bright')\nplt.title('Distribution of Output Classes')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Target Classes', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(25, 16))\n\nfor class_id in sorted(train_df.diagnosis.unique()):\n    for i, (idx, row) in enumerate(train_df.loc[train_df['diagnosis'] == class_id].sample(5).iterrows()):\n        ax = fig.add_subplot(5, 5, class_id * 5 + i + 1, xticks=[], yticks=[])\n        path=f\"{train_dir}{row['id_code']}.png\"\n        image = cv2.imread(path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n\n        plt.imshow(image)\n        ax.set_title('Label: %d-%d-%s' % (class_id, idx, row['id_code']) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's crop and see. I took the function crop_image_from_gray from this kernel: https://www.kaggle.com/ratthachat/aptos-eye-preprocessing-in-diabetic-retinopathy . Here at first if the image is 2 dimensional or 3 dimensional is checked. Regardles of the image being 2D or 3D, the black border is cropped using mask.any(0) and mask.any(1). This method is well explained here: https://codereview.stackexchange.com/questions/132914/crop-black-border-of-image-using-numpy\n\nmask.any(1) or mask.any(axis=1) is applied the mask columnwise. Similarly mask.any(axis=0) applies the mask row-wise. These two boolean arrays are used to index the image and then crop the image (shown in the code below)."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(5, 5))\nfor i in os.listdir(train_dir):\n    image = cv2.imread(train_dir+i)\n    gray_img = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    mask = gray_img>7\n\n    \n    plt.imshow(image[:,:,0][np.ix_(mask.any(1),mask.any(0))])\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def crop_image_from_gray(img,tol=7):\n    if img.ndim ==2:\n        mask = img>tol\n        return img[np.ix_(mask.any(1),mask.any(0))]\n    elif img.ndim==3:\n        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n        mask = gray_img>tol\n        \n        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n        if (check_shape == 0): # image is too dark so that we crop out everything,\n            return img # return original image\n        else:\n            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n                #         print(img1.shape,img2.shape,img3.shape)\n            img = np.stack([img1,img2,img3],axis=-1)\n    #         print(img.shape)\n        return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ImageData(Dataset):\n    def __init__(self, df, data_dir, transform):\n        super().__init__()\n        self.df = df.values\n        self.data_dir = data_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):       \n        img_name,label = self.df[index]\n        \n        img_path = os.path.join(self.data_dir, img_name+'.png')\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = crop_image_from_gray(image)\n        image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n        image = cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , 30) ,-4 ,128)\n        if self.transform:\n            image = self.transform(image)\n        return image, label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_transf = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomRotation((-180, 180)),\n    transforms.RandomHorizontalFlip(p=0.4),\n    transforms.RandomVerticalFlip(p=0.5),\n    #transforms.ColorJitter(brightness=2, contrast=2),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transforms_valid = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_one_fold(i_fold, model, loss_func, optimizer, train_loader, valid_loader):\n    \n    train_fold_results = []\n\n    for epoch in range(N_EPOCHS):\n\n        print('  Epoch {}/{}'.format(epoch + 1, N_EPOCHS))\n        print('  ' + ('-' * 20))\n\n        model.train()\n        tr_loss = 0\n        train_kappa = []\n        optimizer.zero_grad()\n        for ii, (data, target) in enumerate(train_loader):\n\n            images = data.to(device, dtype = torch.float)\n            labels = target.to(device, dtype = torch.float)\n            \n            labels = labels.view(-1,1)\n            optimizer.zero_grad()\n            \n            \n            with torch.set_grad_enabled(True):\n                output = model(images)                \n                loss = loss_func(output, labels)\n                loss.backward()\n\n                optimizer.step()\n            \n            tr_loss += loss.item()\n            \n            y_actual = labels.data.cpu().numpy()\n            y_pred = output[:,-1].detach().cpu().numpy()\n            kappa = cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic')\n            train_kappa.append(kappa)\n        \n        train_kappa_epoch = np.mean(train_kappa)\n            \n\n        # Validate\n        model.eval()\n        \n        val_loss = 0\n        val_preds = None\n        val_labels = None\n        val_kappa = []\n        \n        for ii, (data, target) in enumerate(valid_loader):\n\n\n            images = data.to(device, dtype = torch.float)\n            labels = target.to(device, dtype = torch.float)\n\n            with torch.no_grad():\n                outputs = model(images)\n\n                loss = loss_func(outputs, labels.squeeze(-1))\n                val_loss += loss.item()\n\n            y_actual = labels.data.cpu().numpy()\n            y_pred = outputs[:,-1].detach().cpu().numpy()\n            kappa = cohen_kappa_score(y_actual, y_pred.round(), weights='quadratic')\n            val_kappa.append(kappa)\n\n           \n        val_kappa_epoch = np.mean(val_kappa)\n        train_loss = tr_loss/len(train_loader)\n        valid_loss = val_loss/len(valid_loader)\n        \n        print('Fold: {}, Epoch: {}, Train Loss: {:.6f}, Valid Loss: {:.6f}, Train Kappa: {:.4f}, Valid Kappa: {:.4f}'.format(i_fold, epoch,train_loss, valid_loss, train_kappa_epoch, val_kappa_epoch))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install efficientnet_pytorch\nfrom efficientnet_pytorch import EfficientNet","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model\nI used EfficientNet B3 for this dataset. I could not use Efficient Net B4 or higher as they kept failing due to memory shortage.   "},{"metadata":{"trusted":true},"cell_type":"code","source":"model = EfficientNet.from_name('efficientnet-b3')\nmodel.load_state_dict(torch.load('../input/efficientnet-pytorch/efficientnet-b3-c8376fa2.pth'))\nin_features = model._fc.in_features\nmodel._fc = nn.Linear(in_features, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df.id_code, train_df.diagnosis)):\n    print(\"Fold {}/{}\".format(i_fold + 1, N_FOLDS))\n\n    valid = train_df.iloc[valid_idx]\n    valid.reset_index(drop=True, inplace=True)\n\n    train = train_df.iloc[train_idx]\n    train.reset_index(drop=True, inplace=True)    \n    \n    train_data = ImageData(df = train, data_dir = train_dir, transform = data_transf)\n    dataset_valid = ImageData(df=valid, data_dir = train_dir, transform =transforms_valid)\n\n    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, num_workers=4, shuffle=True)\n    valid_loader = DataLoader(dataset_valid, batch_size=BATCH_SIZE, num_workers=4, shuffle=False)\n    \n    \n    model = model.to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.00015, weight_decay=1e-5)\n    loss_func = nn.MSELoss()\n        \n    train_one_fold(i_fold, model, loss_func, optimizer, train_loader, valid_loader)\n\ntorch.save(model.state_dict(), 'aptos_3_fold_weights.pth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}